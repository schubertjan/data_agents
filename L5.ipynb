{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Lesson 5: Measure Agent‚Äôs GPA\n",
    "\n",
    "Goal-Plan-Act Alignment unlocks useful feedback on how to improve the effectiveness of your agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "height": 149
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "\n",
    "load_dotenv(override=True)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "<p> üíª &nbsp; <b>To access <code>requirements.txt</code>, <code>env.template</code>, <code>prompts.py</code>, and <code>helper.py</code> files:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook 2) click on <em>\"Open\"</em>.\n",
    "\n",
    "<p> ‚¨á &nbsp; <b>Download Notebooks:</b> 1) click on the <em>\"File\"</em> option on the top menu of the notebook and then 2) click on <em>\"Download as\"</em> and select <em>\"Notebook (.ipynb)\"</em>.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Goal-Plan-Act alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agents are most effective when acting in alignment with a high-quality plan. For that reason, you can identify common failure modes stemming from misalignment between the Goal, the Plan, and the agent's Actions.\n",
    "\n",
    "Then, through careful criteria and a strong LLM judge, you can develop evaluators to detect these common agent failure modes and assess separable dimensions of agent quality.\n",
    "\n",
    "We will start with some illustrative examples of common failure modes and how we can identify issues in goal-plan-action alignment with an LLM as a judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "gpa_eval_provider = OpenAI(model_engine=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure mode 1: Plan Quality\n",
    "\n",
    "The starting point for an agent is its plan. Without a high-quality plan, the agent has little hope of succeeding. You start by assessing the plan to ensure it is well-structured and aligned with the goal.\n",
    "\n",
    "To demonstrate, consider the query: \"Which sales leads should we prioritize this week, and what specific action items should we take for each?\" and the plan below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "height": 268
   },
   "outputs": [],
   "source": [
    "goal_and_plan = \"\"\"\n",
    "User Query: Which sales leads should we prioritize this week, \n",
    "and what specific action items should we take for each?\n",
    "\n",
    "Plan:\n",
    "\n",
    "1. Pull all sales leads from the past 12 months from the CRM.\n",
    "\n",
    "2. For the largest 20 leads, compile any notes, call logs, \n",
    "and related tasks from the CRM.\n",
    "\n",
    "3. Summarize each lead‚Äôs current stage in the pipeline.\n",
    "\n",
    "4. Present the summary and recommendations in a single table.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "from trulens.core import Feedback\n",
    "from trulens.core.feedback.selector import Selector\n",
    "\n",
    "# Goal-Plan-Act: Plan quality\n",
    "f_plan_quality = Feedback(\n",
    "    gpa_eval_provider.plan_quality_with_cot_reasons,\n",
    "    name=\"Plan Quality\",\n",
    ").on({\n",
    "    \"trace\": Selector(trace_level=True),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "height": 115
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3333333333333333 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-size: 15px; word-wrap: break-word; width: 800px;\">Criteria: The plan generally addresses the query and is feasible, but lacks explicit justification for some steps, omits critical details on prioritization and action items, and does not fully articulate rationale for choices such as time frame and lead selection.<br><br>Supporting Evidence: Step 1 gathers relevant data but uses a 12-month window without tying it to the user's 'this week' focus. Step 2 targets the largest 20 leads but does not define 'largest' or justify the number. Step 3 is necessary for prioritization. Step 4 presents results clearly but omits how recommendations and action items are generated. The plan lacks explicit prioritization criteria and does not detail how to create specific action items, which are critical omissions.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper import display_eval_reason\n",
    "\n",
    "score, reason = f_plan_quality(goal_and_plan)\n",
    "\n",
    "print(f\"Score: {score} \\n\")\n",
    "display_eval_reason(reason['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Summary:** \n",
    "\n",
    "Why is the first plan low quality?\n",
    "\n",
    "- Vague selection: \"Pull all sales leads from the past 12 months\" lacks urgency constraints tied to the goal.\n",
    "- Weak prioritization: \"largest 20\" ignores lead score, stage urgency, or upcoming deadlines.\n",
    "- Missing actionability: no instructions to create specific next actions or owners.\n",
    "- Output not specific: \"single table\" without required fields tied to the goal.\n",
    "\n",
    "What the evaluator flags:\n",
    "- Specific constraints, measurable outputs, and sequencing tied to goal. The \"better plan\" adds these, raising the score.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "height": 538
   },
   "outputs": [],
   "source": [
    "goal_and_better_plan = \"\"\"\n",
    "User Query: Which sales leads should we prioritize this week, \n",
    "and what specific action items should we take for each?\n",
    "\n",
    "Plan:\n",
    "\n",
    "1. Pull all leads with open opportunities from the CRM that have \n",
    "a next action date within the next 14 days or no next action assigned.\n",
    "\n",
    "2. Filter to leads with deal value > $10k or high lead score.\n",
    "\n",
    "3. Sort by deal stage urgency (e.g., close date approaching, \n",
    "at risk of going cold) and potential revenue impact.\n",
    "\n",
    "4. For each prioritized lead:\n",
    "\n",
    "5. Retrieve latest interaction notes, key decision-maker info, \n",
    "and current blockers.\n",
    "\n",
    "6.  Identify overdue or missing action items.\n",
    "\n",
    "7. Propose specific, high-impact next steps (e.g., schedule product demo, \n",
    "send proposal revision, escalate to sales manager).\n",
    "\n",
    "8. Group recommendations into this week‚Äôs priority list with owner \n",
    "assignments and deadlines.\n",
    "\n",
    "9. Present results in a table with columns: Lead Name, Value, Stage, \n",
    "Urgency Score, Next Action, Due Date, Owner.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-size: 15px; word-wrap: break-word; width: 800px;\">Criteria: Plan is well-structured, optimal, and directly addresses the user's query with clear, actionable, and logical steps; each step is justified, necessary, and sufficiently detailed; no replanning required.<br><br>Supporting Evidence: Plan addresses both prioritizing leads and specifying action items. Steps logically filter and sort leads using relevant criteria, then detail how to generate actionable next steps, including context gathering and proposing actions. Recommendations are grouped with owners and deadlines, and output format is clear. All steps are necessary and justified; none are superfluous or unclear. No replanning occurs.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score, reason = f_plan_quality(goal_and_better_plan)\n",
    "\n",
    "print(f\"Score: {score} \\n\")\n",
    "display_eval_reason(reason['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Bad vs Good (Plan Quality)**\n",
    "\n",
    "- **Bad**: Vague steps, no thresholds, unclear output, missing prioritization.\n",
    "- **Good**: Explicit filters and thresholds, prioritization logic, and defined output schema.\n",
    "\n",
    "\n",
    "In reality, you cannot make direct edits to the plan, nor to the actions of the agent. You can, however, make adjustments to the agent to help guide it towards higher goal-plan-action alignment. In the next lesson, you will get your hands on making such improvements.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure mode 2: Plan Adherence\n",
    "\n",
    "Once a high-quality plan is developed, the agent must follow it. Plan adherence checks to make sure the agent's action is aligned with the plan. Consider the following execution trace and the plan you developed before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick checklist: Plan Quality**\n",
    "\n",
    "- Tightly aligned to the user goal.\n",
    "- Specific selection criteria and thresholds.\n",
    "- Clear step ordering and ownership when relevant.\n",
    "- Concrete outputs (schema/columns) and success criteria.\n",
    "- Uses the right agents/tools for each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "height": 164
   },
   "outputs": [],
   "source": [
    "agent_actions = \"\"\"\n",
    "[STEP 1] Pulled all open opportunities from the CRM without applying a next action date filter.\n",
    "[STEP 2] Applied deal value filter only; skipped the lead score filter.\n",
    "[STEP 3] Sorted leads solely by deal value (descending).\n",
    "[STEP 4] Retrieved latest notes and contact names but skipped blockers.\n",
    "[STEP 5] Listed the CRM‚Äôs existing \"next action\" field without review or update.\n",
    "[STEP 6] Output a table with Lead Name, Value, Stage, and Next Action.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "plan_and_agent_actions = goal_and_better_plan + agent_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Goal-Plan-Act: Plan adherence\n",
    "f_plan_adherence = Feedback(\n",
    "    gpa_eval_provider.plan_adherence_with_cot_reasons,\n",
    "    name=\"Plan Adherence\",\n",
    ").on({\n",
    "    \"trace\": Selector(trace_level=True),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-size: 15px; word-wrap: break-word; width: 800px;\">Criteria: Multiple planned steps were omitted, performed out of order, or replaced with unplanned actions. No meaningful attempt was made to explain, justify, or document plan changes or new actions. The plan was largely ignored or disregarded in execution, or steps were not completed as intended. If replanning was necessary, the revised plan was not followed.<br><br>Supporting Evidence: Step 1: Did not apply the next action date filter as specified. Step 2: Only applied deal value filter; skipped high lead score filter. Step 3: Sorted only by deal value, not by deal stage urgency or revenue impact. Step 4: Retrieved latest notes and contact names but skipped blockers. Step 5: Listed existing 'next action' field without review or update, skipping identification of overdue/missing actions and proposing new next steps. Step 6: Output table omitted required columns and did not group recommendations or assign owners/deadlines. No justification for deviations.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score, reason = f_plan_adherence(plan_and_agent_actions)\n",
    "\n",
    "print(f\"Score: {score} \\n\")\n",
    "display_eval_reason(reason['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Why does this trace violate the plan?\n",
    "\n",
    "- Missing the date filter from Step 1.\n",
    "  - Plan: \"next action within 14 days OR no next action\"\n",
    "  - Trace: \"Pulled all open opportunities... without applying a next action date filter.\"\n",
    "- Partial filter in Step 2.\n",
    "  - Plan: value > $10k OR high lead score\n",
    "  - Trace: \"Applied deal value filter only; skipped the lead score filter.\"\n",
    "- Output mismatch.\n",
    "  - Plan: include Urgency Score, Due Date, Owner\n",
    "  - Trace: table has only Lead Name, Value, Stage, Next Action\n",
    "\n",
    "What the evaluator flags:\n",
    "- Each plan requirement should appear in the trace. Omissions above directly lower adherence.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "height": 251
   },
   "outputs": [],
   "source": [
    "better_agent_actions = \"\"\"[STEP 1] Pulled all leads with open \n",
    "opportunities and either a next action date within 14 days or no next \n",
    "action assigned.\n",
    "[STEP 2] Filtered to leads with deal value over $10k or high lead score.\n",
    "[STEP 3] Sorted leads by deal stage urgency and potential revenue impact.\n",
    "[STEP 4] Retrieved latest notes, key decision-maker info, and identified \n",
    "any blockers.\n",
    "[STEP 5] Created updated, specific next actions for each lead based on \n",
    "context. \n",
    "[STEP 6] Group recommendations into this week‚Äôs priority list with owner \n",
    "assignments and deadlines.\n",
    "[STEP 7] Output a table with Lead Name, Value, Stage, Urgency Score, \n",
    "Next Action, Due Date, and Owner.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "height": 45
   },
   "outputs": [],
   "source": [
    "plan_and_better_agent_actions = goal_and_better_plan + better_agent_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-size: 15px; word-wrap: break-word; width: 800px;\">Criteria: Each step in the plan was executed and completed correctly and in entirety. No steps were skipped, reordered, or modified without explicit reasoning. No deviations or omissions.<br><br>Supporting Evidence: All seven steps were executed as planned, in order and in entirety. No steps were skipped, reordered, or modified. No evidence of deviation or omission.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score, reason = f_plan_adherence(plan_and_better_agent_actions)\n",
    "\n",
    "print(f\"Score: {score} \\n\")\n",
    "display_eval_reason(reason['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure mode 3: Execution Efficiency\n",
    "\n",
    "Even when acting in logical ways that adhere to a high-quality plan, agents can act in overly defensive ways that reduce efficiency unnecessarily.\n",
    "\n",
    "Evaluating execution efficiency helps to flag redundancies, preventable mistakes, and excessive error handling.\n",
    "\n",
    "Consider a new execution trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "height": 438
   },
   "outputs": [],
   "source": [
    "agent_actions = \"\"\"\n",
    "[STEP 1] Pulled all leads with open opportunities and either a next \n",
    "action date within 14 days or no next action assigned.\n",
    "    ‚Üí Retrieved 96 leads.\n",
    "\n",
    "[STEP 2] Filtered to leads with deal value over $10k or high lead score.\n",
    "    ‚Üí Applied filter, yielding 54 leads.\n",
    "\n",
    "[STEP 3] Sorted leads by deal stage urgency and potential revenue impact.\n",
    "    ‚Üí High-value late-stage leads ranked highest.\n",
    "\n",
    "[STEP 4] Retrieved latest notes, key decision-maker info, and blockers.\n",
    "    ‚Üí Retrieved notes from both the CRM API and a cached export for one \n",
    "    lead to ‚Äúdouble-check‚Äù consistency.\n",
    "\n",
    "[STEP 5] Created updated, specific next actions for each lead based on \n",
    "context.\n",
    "    ‚Üí Example: Lead A ‚Äî ‚ÄúSchedule demo and confirm final pricing‚Äù; Lead \n",
    "    B ‚Äî ‚ÄúFollow up on proposal feedback by Thursday.‚Äù\n",
    "\n",
    "[STEP 6] Output a table with Lead Name, Value, Stage, Urgency Score, \n",
    "Next Action, Due Date, and Owner.\n",
    "    ‚Üí Exported table to both XLSX and CSV formats, though only one \n",
    "    format was requested.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Goal-Plan-Act: Execution efficiency of trace\n",
    "f_execution_efficiency = Feedback(\n",
    "    gpa_eval_provider.execution_efficiency_with_cot_reasons,\n",
    "    name=\"Execution Efficiency\",\n",
    ").on({\n",
    "    \"trace\": Selector(trace_level=True),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.6666666666666666 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-size: 15px; word-wrap: break-word; width: 800px;\">Criteria: Efficiency of execution: streamlined actions, no unnecessary repetition, minimal resource waste, lean error handling, and non-redundant verification.<br><br>Supporting Evidence: Step 4 involved redundant retrieval of notes from both CRM API and cached export for one lead, introducing minor inefficiency. Step 6 exported data to both XLSX and CSV formats when only one was needed, resulting in unnecessary resource use. All other steps were logical and direct, with no major backtracking or excessive error handling.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score, reason = f_execution_efficiency(agent_actions)\n",
    "\n",
    "print(f\"Score: {score} \\n\")\n",
    "display_eval_reason(reason['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Why is this trace inefficient?\n",
    "\n",
    "- Duplicate work: re-applied the same filter.\n",
    "  - Trace: \"Accidentally re-applied the same filter twice...\"\n",
    "  - Impact: extra compute with no new signal.\n",
    "- Redundant retrieval: double-fetched notes to \"double-check.\"\n",
    "  - Trace: \"Retrieved notes from both the CRM API and a cached export...\"\n",
    "  - Impact: unnecessary network/IO; one source is sufficient.\n",
    "- Unrequested outputs: exported multiple formats.\n",
    "  - Trace: \"Exported table to both XLSX and CSV...\"\n",
    "  - Impact: violates YAGNI; adds time and clutter.\n",
    "\n",
    "What the evaluator flags:\n",
    "- Looks for repeated steps, unnecessary retries, and outputs beyond the plan/request.\n",
    "- All three points above directly lower the efficiency score.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Bad vs Good (Execution Efficiency)**\n",
    "\n",
    "- **Bad**: Re-applies filters multiple times; double-fetches the same notes to \"be safe\"; exports to extra formats not requested; retries on transient warnings without need.\n",
    "- **Good**: Applies each filter exactly once in a single pass; reuses cached results instead of refetching; outputs only the requested format; handles errors proportionally (warn ‚Üí continue; error ‚Üí fix once and proceed).\n",
    "\n",
    "Small rewrite example:\n",
    "- Bad: \"Applied deal value filter, then re-applied to confirm.\"\n",
    "- Good: \"Applied combined filters: value > $10k OR high lead score (single pass).\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Failure Mode 4: Logical Inconsistency\n",
    "\n",
    "Agents' actions can suffer from contradictions, ungrounded assumptions, and logical flaws.\n",
    "\n",
    "Let's consider a different execution trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "height": 455
   },
   "outputs": [],
   "source": [
    "agent_actions = \"\"\"\n",
    "[STEP 1] Pulled all leads with open opportunities and either a next \n",
    "action date within 14 days or no next action assigned.\n",
    "    ‚Üí Retrieved 96 leads, including recent follow-ups and a few older \n",
    "    records from early last year.\n",
    "\n",
    "[STEP 2] Filtered to leads with deal value over $10k or high lead score.\n",
    "    ‚Üí Resulted in 113 leads after applying filters.\n",
    "\n",
    "[STEP 3] Sorted leads by deal stage urgency and potential revenue impact.\n",
    "    ‚Üí Leads with minimal recent engagement ranked highly due to their \n",
    "    projected close dates in Q3.\n",
    "\n",
    "[STEP 4] Retrieved latest notes, key decision-maker info, and blockers.\n",
    "    ‚Üí Several leads show ‚ÄúTBD‚Äù for decision-maker but still have active \n",
    "    next steps assigned.\n",
    "\n",
    "[STEP 5] Created updated, specific next actions for each lead based on \n",
    "context.\n",
    "    ‚Üí Example: Lead A ‚Äî ‚ÄúSchedule demo and confirm final pricing‚Äù; Lead \n",
    "    B ‚Äî ‚ÄúWait for proposal feedback before scheduling demo.‚Äù\n",
    "\n",
    "[STEP 6] Output a table with Lead Name, Value, Stage, Urgency Score, \n",
    "Next Action, Due Date, and Owner.\n",
    "    ‚Üí Due dates range from last week to the end of the current month.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "# Goal-Plan-Act: Logical consistency of trace\n",
    "f_logical_consistency = Feedback(\n",
    "    gpa_eval_provider.logical_consistency_with_cot_reasons,\n",
    "    name=\"Logical Consistency\",\n",
    ").on({\n",
    "    \"trace\": Selector(trace_level=True),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.3333333333333333 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"font-size: 15px; word-wrap: break-word; width: 800px;\">Criteria: Logical consistency of the trace, including plan and execution; actions and claims should be justified by prior context, corrections acknowledged, instructions followed, and reasoning coherent.<br><br>Supporting Evidence: [STEP 1] and [STEP 2]: Contradiction in lead counts (96 to 113) without explanation. [STEP 3]: Sorting rationale not fully justified. [STEP 4]: 'TBD' decision-maker with active next steps unexplained. [STEP 5]: Next actions consistent. [STEP 6]: Table output consistent, but past due dates not justified.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "score, reason = f_logical_consistency(agent_actions)\n",
    "\n",
    "print(f\"Score: {score} \\n\")\n",
    "display_eval_reason(reason['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Why is this trace inconsistent?\n",
    "\n",
    "- Count contradiction after filtering.\n",
    "  - Plan step: filter to leads >$10k OR high lead score.\n",
    "  - Trace: 96 ‚Üí 113 leads after filter.\n",
    "  - Why it's a failure: filters should not increase the set; this implies an inconsistency.\n",
    "- Action vs state mismatch.\n",
    "  - Trace: decision-maker is \"TBD\" but \"active next steps\" are assigned.\n",
    "  - Why it's a failure: missing prerequisite info for the assigned action.\n",
    "\n",
    "What the evaluator flags:\n",
    "- Numerical sanity across steps; contradictions or impossible transitions.\n",
    "- The two points above directly reduce the consistency score.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff6ff; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "\n",
    "**Bad vs Good (Logical Consistency)**\n",
    "\n",
    "- **Bad**: Counts grow after applying stricter filters; assigns next steps when decision-maker is \"TBD\"; contradicts earlier statements.\n",
    "- **Good**: Counts decrease or stay the same after filters; next steps match available context; statements remain consistent with prior steps.\n",
    "\n",
    "Small rewrite example:\n",
    "- Bad: \"Resulted in 113 leads after applying filters to 96 leads.\"\n",
    "- Good: \"Filtered 96 ‚Üí 54 leads based on value > $10k OR high lead score.\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap: Common Failure Modes and Fixes\n",
    "\n",
    "- **Plan Quality**: Ensure the plan is specific, feasible, and tied to the goal. Include ordering and explicit outputs.\n",
    "- **Plan Adherence**: Execute each plan step as written. Apply filters and produce the exact requested outputs; report any deviations.\n",
    "- **Execution Efficiency**: Avoid redundant work and overly defensive retries; do only what is necessary to achieve the goal.\n",
    "- **Logical Consistency**: Keep counts and facts consistent across steps; avoid contradictions and unsupported claims.\n",
    "\n",
    "Use these checklists above to quickly self-audit traces before running full evaluations. \n",
    "\n",
    "**Let's apply these evaluations to the data agent.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Create TruLens session for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶ë Initialized with db url sqlite:///default.sqlite .\n",
      "üõë Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n",
      "‚úÖ experimental Feature.OTEL_TRACING enabled.\n",
      "üîí experimental Feature.OTEL_TRACING is enabled and cannot be changed.\n"
     ]
    }
   ],
   "source": [
    "from trulens.core.session import TruSession\n",
    "from trulens.core.database.connector.default import DefaultDBConnector\n",
    "\n",
    "# Initialize connector with SQLite database one folder back\n",
    "connector = DefaultDBConnector(database_url=\"sqlite:///default.sqlite\")\n",
    "\n",
    "# Create TruSession with the custom connector\n",
    "session = TruSession(connector=connector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "height": 283
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "from helper import State, planner_node, executor_node, cortex_agents_research_node, web_research_node, chart_node, chart_summary_node, synthesizer_node\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"planner\", planner_node)\n",
    "workflow.add_node(\"executor\", executor_node)\n",
    "workflow.add_node(\"web_researcher\", web_research_node)\n",
    "workflow.add_node(\"cortex_researcher\", cortex_agents_research_node)\n",
    "workflow.add_node(\"chart_generator\", chart_node)\n",
    "workflow.add_node(\"chart_summarizer\", chart_summary_node)\n",
    "workflow.add_node(\"synthesizer\", synthesizer_node)\n",
    "\n",
    "workflow.add_edge(START, \"planner\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Register the agent with TruLens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> \n",
    "    <p>üö® &nbsp; In this notebook, you are directly provided with the results obtained during filming. This is to help eliminate waiting time, and to prevent potential rate limit errors that might occur in this learning environment (this learning environment is constrained, and the GPA evaluation metrics consume a significant number of tokens). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code that registers the agent with TruLens:\n",
    "```python\n",
    "from trulens.apps.langgraph import TruGraph\n",
    "from helper import f_answer_relevance, f_context_relevance, f_groundedness\n",
    "\n",
    "tru_recorder = TruGraph(\n",
    "    graph,\n",
    "    app_name=\"Sales Data Agent\",\n",
    "    app_version=\"L5: Base\",\n",
    "    feedbacks=[\n",
    "        f_answer_relevance,\n",
    "        f_context_relevance,\n",
    "        f_groundedness,\n",
    "        f_plan_quality,\n",
    "        f_plan_adherence,\n",
    "        f_execution_efficiency,\n",
    "        f_logical_consistency,\n",
    "    ],\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Record agent usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#f7fff8; padding:15px; border-width:3px; border-color:#e0f0e0; border-style:solid; border-radius:6px\"> \n",
    "    <p>üö® &nbsp; <b>Run Results:</b> In this notebook, you are directly provided with the results obtained during filming. This is to help eliminate waiting time, and to prevent potential rate limit errors that might occur in this learning environment (this learning environment is constrained, and the GPA evaluation metrics consume a significant number of tokens).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for query 1:**\n",
    "``` python\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    query = \"What are our top 3 client deals? Chart the deal value for each.\"\n",
    "    print(f\"Query: {query}\")\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \n",
    "                                   \"chart_generator\", \"chart_summarizer\", \n",
    "                                   \"synthesizer\"],\n",
    "            }\n",
    "    graph.invoke(state)\n",
    "    print(\"--------------------------------\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are our top 3 client deals? Chart the deal value for each.\n",
      "\n",
      "Output: The chart saved at `top_3_client_deals.png` reveals the top 3 client deals by value: Client A had the largest deal valued at $500,000, followed by Client B at $450,000, and Client C at $400,000.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "records, feedback = session.get_records_and_feedback()\n",
    "print(f\"Query: {records.iloc[0]['input']}\\n\")\n",
    "print(f\"Output: {records.iloc[0]['output']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for query 2:**\n",
    "```python\n",
    "with tru_recorder as recording:\n",
    "    query = \"Identify our pending deals, research if they may be experiencing regulatory changes, and using the meeting notes for each customer, provide a new value proposition for each given the regulatory changes.\"\n",
    "    print(f\"Query: {query}\")\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \n",
    "                                   \"chart_generator\", \"chart_summarizer\", \n",
    "                                   \"synthesizer\"],\n",
    "            }\n",
    "    graph.invoke(state)\n",
    "\n",
    "    print(\"--------------------------------\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Identify our pending deals, research if they may be experiencing regulatory changes, and using the meeting notes for each customer, provide a new value proposition for each given the regulatory changes.\n",
      "\n",
      "Output: - **TechCorp Inc.:** No specific regulatory changes identified, but generalized tech sector issues like import/export compliance might be relevant. Update the value proposition to emphasize seamless integration capabilities and cost-effective compatibility with existing systems to address potential budget constraints.\n",
      "  \n",
      "- **GlobalTrade Inc.:** Affected by shifts in trade policies and tariffs; propose enhanced logistics and supply chain solutions to optimize efficiency amidst regulatory pressure.\n",
      "  \n",
      "- **UpgradeNow Corp.:** No direct regulatory changes found. Continue to focus on strengthening analytics capabilities and system scalability based on user satisfaction and potential for upgrades.\n",
      "\n",
      "- **SmallBiz Solutions:** Impact from state-level wage and employment regulations; highlight products that streamline compliance efforts and automate financial reporting to address these concerns.\n",
      "\n",
      "- **SecureBank Ltd.:** Facing regulatory changes in financial compliance, particularly in fintech. Offer advanced security features that strengthen compliance with new oversight requirements, focusing on integrated security solutions in digital banking.\n",
      "\n",
      "Citations: [...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {records.iloc[1]['input']}\\n\")\n",
    "print(f\"Output: {records.iloc[1]['output']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code for query 3**\n",
    "```python\n",
    "with tru_recorder as recording:\n",
    "    query = \"Identify our largest client deal, then find important topics in the meeting notes with that company, and find a news article related to the important topics discussed.\"\n",
    "    print(f\"Query: {query}\")\n",
    "    state = {\n",
    "                \"messages\": [HumanMessage(content=query)],\n",
    "                \"user_query\": query,\n",
    "                \"enabled_agents\": [\"cortex_researcher\", \"web_researcher\", \n",
    "                                   \"chart_generator\", \"chart_summarizer\", \n",
    "                                   \"synthesizer\"],\n",
    "            }\n",
    "    graph.invoke(state)\n",
    "\n",
    "    print(\"--------------------------------\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "height": 47
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Identify our largest client deal, then find important topics in the meeting notes with that company, and find a news article related to the important topics discussed.\n",
      "\n",
      "Output: - **Largest Client Deal**: The largest client deal is with FastTrack Ltd, valued at $180,000, closed by Sarah Johnson on February 12, 2024, involving the Premium Security product line.\n",
      "- **Missing Information**: Details on important topics discussed in meetings with FastTrack Ltd are not provided. Please supply topics from meeting notes to find related news articles.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: {records.iloc[2]['input']}\\n\")\n",
    "print(f\"Output: {records.iloc[2]['output']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Launch the TruLens dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Make sure to click on the second link (not the localhost) to open the TruLens dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a896ae325a1a452687d3b82d09051e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trulens.dashboard import run_dashboard\n",
    "import os\n",
    "str_port = 8004\n",
    "_ = run_dashboard(port=str_port)\n",
    "print(os.environ['DLAI_LOCAL_URL'].format(port=str_port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
